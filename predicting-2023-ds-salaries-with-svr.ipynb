{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7279972,"sourceType":"datasetVersion","datasetId":4213326}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I've chosen to use Support Vector Regression for a few main reasons:\n\n1. Memory efficiency\n\n2. Ability to work well in high dimensional spaces (Due to high numbers of unique values in columns)\n\n3. Ability to handle non-linear regression using the *kernal trick*\n\n4. Robustness in preventing overfitting and resistance against outliers\n\nI'm chosing to use only 2023 data given how volatile the job market has been and the recent influence of hype in Data Science and its effects on salary variability. To reduce this noise, I'm only examining the most recent data the dataset has provided.","metadata":{}},{"cell_type":"code","source":"#Import libraries for analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\n#from sklearn.metrics import r2_score, root_mean_squared_error #Kaggle does not support r2 or NRSME\n#Download code and run on IDE for live metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read CSV and print first 5 rows\ndf = pd.read_csv('/kaggle/input/jobs-in-data/jobs_in_data.csv')\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop unneccessary columns\ndf.drop(['salary_currency','salary'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Examine dataframe\nprint(df.describe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filter for jobs in 2023 based on row value\ndf = df[df['work_year'] == 2023]\n\n#Drop work year to remove redundancy\ndf.drop(['work_year'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One-Hot encode categorical values and replace boolean values with 0,1\ndf = pd.get_dummies(df, columns=['job_category','work_setting','company_location','employee_residence',\\\n                                 'company_size','employment_type','experience_level','job_title'])\ndf.replace(to_replace = True, value = 1, inplace = True)\ndf.replace(to_replace = False, value = 0, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if OHE worked and data types are all numbers\nprint(df.info())\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize target variable to see distribution\nplt.hist(df.salary_in_usd)\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transform target salary data to normalize right skewness and visualize\ndf.salary_in_usd = np.log(df.salary_in_usd)\nplt.hist(df.salary_in_usd)\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get x and y variables\nx = df.drop(['salary_in_usd'], axis=1)\ny = df.salary_in_usd\n\n#Split dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.9, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#NOTE: LONG RUNTIME\n#Find optimal parameters using GridSearch\n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']}\n\ngrid = GridSearchCV(SVR(), param_grid, refit=True, verbose=3)\n\n# fitting the model for grid search\ngrid.fit(x_train, y_train)\n\n# print best parameter after tuning\nprint(grid.best_params_)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create model using parameters from GridSearch\nmodel = SVR(C=1, gamma=.1, kernel='rbf')\n\n#Fit model\nmodel.fit(x_train,y_train)\n\n#Create y_pred to score model\ny_pred = model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#Find and print r2 score & normalized mean squared error (THIS)\nr2 = metrics.r2_score(y_test,y_pred)\nNRMSE = metrics.root_mean_squared_error(y_test,y_pred)\nprint(f\"R2 score: {r2}\")\nprint(f\"NRMSE score: {NRMSE / (y.max() - y.min())}\")\n'''\n#Code for Normalized Root Mean Squared Error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R2 score: 0.5102\n\nNRSME score: 0.0983\n\nAverage variance in prediction: 9.83% \n\nThe model captures 51.02% of the data","metadata":{}},{"cell_type":"code","source":"#Plot predictions against actual values\nplt.scatter(np.exp(y_test),np.exp(y_pred), alpha=.2)\nplt.xlabel('Actual Value')\nplt.ylabel('Predicted Value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Max and min over/under estimations, show a sample of the data vs. predictions\nprint(\"Max Overestimation\")\nprint((np.exp(y_pred) / np.exp(y_test)).max())\nprint()\nprint(\"Max Underestimation\")\nprint((np.exp(y_pred) / np.exp(y_test)).min()) \nprint()\nprint(\"Sample of Predictions:\")\nprint((np.exp(y_pred) / np.exp(y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find Squared Variance of test and prediction classes\nprint(f\"Squared Variance of actual data: \", np.sqrt(np.exp((y_test).var())))\nprint(f\"Squared Variance of predicted data: \", np.sqrt(np.exp((y_pred).var())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predicted values have a lower standard deviation compared to the actual salary values, indicating that our algothrim suffers from highly variable data and predicted values will be closer to the mean.\n\nFurther data collections should include more continuous data such as years of experience or more categorical values (Degree Level, Skills Desired) to increase the complexity and accuracy of the model.","metadata":{}}]}